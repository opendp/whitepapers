\documentclass[11pt,a4paper]{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{enumerate} 
\usepackage{physics}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{graphicx}
\hypersetup{colorlinks,
    linkcolor=blue,
    citecolor=blue,      
    urlcolor=blue,
    %linktoc=none
}

\oddsidemargin0.1cm 
\evensidemargin0.8cm
\textheight22.7cm 
\textwidth15cm \topmargin-0.5cm

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem*{theorem-non}{Theorem}

\theoremstyle{definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{observation}{Observation}
\newtheorem{note}[theorem]{Note}
\newtheorem{hope}{Hope}
\newtheorem{warning}{Warning}
\newtheorem{problem}{Problem}
\newtheorem{fear}{Fear}
\newtheorem{question}{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{claim}{Claim}

\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\horizline}{\noindent\rule{\textwidth}{1pt}}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\newcommand{\MultiSet}{\mathrm{MultiSet}}
\newcommand{\MultiSets}{\mathrm{MultiSets}}
\newcommand{\Vect}{\mathrm{Vec}}
\newcommand{\len}{\mathrm{len}}
\newcommand{\din}{d_{in}}
\newcommand{\dout}{d_{out}}
\newcommand{\Relation}{\mathrm{relation}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\True}{\texttt{True}}
\newcommand{\False}{\texttt{False}}
\newcommand{\clamp}{\texttt{clamp}}
\newcommand{\questionc}[1]{\textcolor{red}{\textbf{Question:} #1}}
\newcommand{\T}{\texttt{T}}
\newcommand{\Lap}{\mathrm{Lap}}
\newcommand{\M}{\mathcal{M}}

\newcommand{\silvia}[1]{{ {\color{blue}{(silvia)~#1}}}}
\newcommand{\grace}[1]{{ {\color{purple}{(grace)~#1}}}}
\newcommand{\connor}[1]{{ {\color{teal}{(connor)~#1}}}}

\newcommand{\todo}{{\textcolor{red}{TODO }}}

\title{List of definitions used in the proofs}
\author{S\'ilvia Casacuberta, Grace Tian, and Connor Wagaman}
\date{Summer 2021}

\begin{document}

\maketitle

We use the following guideline: if a term appears in the preconditions \& pseudocode section, then this term is defined in the ``List of definitions used in the pseudocode" document. Otherwise, it appears in the ``List of definitions used in the proofs" document. 

In both cases, we maintain the terms in alphabetical order within each section. `TODOs'' should be included at the end of the corresponding section. On the other hand, ``TODOs'' which better specify an already-defined term should be included immediately following the definition of that term. Examples should never be part of the definition, but we encourage their use right after the definition of a term.

Note that, as of September 4, 2021, the definitions in sections 5+ are not used in the most finalized proof documents. Please leave feedback, but note that the correctness of the following sections does not impact the correctness of the proofs that are most finalized.

\tableofcontents

\section*{List of terms that have not yet been added}
\begin{itemize}
\item Generalization of the path property and the corresponding proofs
\item Better definition for multisets, and an explanation of the bijection between multisets and histograms
\item Clarification sentence on the stability relation
\item Approach with floating point. New from 3/8: explain the different rounding modes in MPFR and which is the preferred one (round to nearest). Assume idealized model for sampling and the discretized versions.
\item Explain unification of vector and scalar versions in the case of L1 distance (e.g., Laplace).
\end{itemize}

\section{Mathematical operators}
The notation we are using for various mathematical operations is\footnote{As of June 24, 2021.} inspired by section 2 of \url{https://hal-ens-lyon.archives-ouvertes.fr/ensl-01529804/file/crlibm.pdf}. (The notation may change in the future.) We plan to use a similar standardized notation for describing the semantics of \href{https://www.mpfr.org/}{MPFR}.\footnote{Library most likely used in OpenDP to deal with floating point arithmetic with certain rounding modes.}

\begin{definition}[$+, -, \times$]
    The symbols $+$, $-$, and $\times$ represent the usual mathematical operations of addition, subtraction, and multiplication, respectively.
\end{definition}

\begin{definition}[$\oplus, \ominus, \otimes$]
    The symbols $\oplus$, $\ominus$, and $\otimes$ denote the corresponding operations on the associated Rust type (rather than on the real numbers).
\end{definition}

\begin{definition}[$\rightarrow$]
    Let $\mathcal{X}$ and $\mathcal{Y}$ be domains. For the deterministic function $f:\mathcal{X}\rightarrow\mathcal{Y}$, the arrow $\rightarrow$ represents a deterministic mapping from domain $\mathcal{X}$ to domain $\mathcal{Y}$.
\end{definition}

\begin{definition}[$\rightsquigarrow$]
    Let $\mathcal{X}$ and $\mathcal{Y}$ be domains. For the randomized function $f:\mathcal{X}\rightsquigarrow\mathcal{Y}$, the squiggly arrow $\rightsquigarrow$ represents a randomized mapping from domain $\mathcal{X}$ to domain $\mathcal{Y}$.
\end{definition}

%\begin{definition}[\texttt{val}]
    %Given a Rust value $x$, the function \texttt{val(x)} returns the actual (real or integer) number represented by the Rust value $x$.
%\end{definition}

\horizline

\subsection{Notes, todos, questions}

\section{Data Representation}

\begin{definition}[Vector]
A vector $v$ is an ordered list of objects.
\end{definition}

A vector is not necessarily Rust vector, but an idealized data container with some notion of records and ordering.

\begin{definition}[Set]
A set is an unordered list of objects.
\end{definition}

\begin{definition}[Multiset]
A multiset is a modification of the notion of a set which, unlike a set, allows for repetitions for each of its elements. The number of repetitions of an element in the multiset corresponds to the \textit{multiplicity} of that element.
\end{definition}

\begin{remark}
    We use the notation $\MultiSets(\mathcal{X})$ to refer to a multiset drawn from the domain of all multisets with elements from domain $\mathcal{X}$. We use the notation $\MultiSet(v)$ to refer to the multiset interpretation (unordered list with multiplicities) of vector $v$.
\end{remark}

\begin{remark}
The distinction between multisets and vectors is relevant because vectors are ordered objects whereas multisets are not. In the OpenDP library, all datasets are represented as vector domains, and therefore for any vector $v$ we need to use the notation $\MultiSet(v)$ when referring to its multiset representation to indicate that ordering should be dropped. 
\end{remark}

\begin{example}
    For $\MultiSet(2, 3, 3, 5, 5, 5 )$, element 2 has multiplicity 1, element 3 has multiplicity 2, and element 5 has multiplicity 3.
\end{example}

\iffalse
\begin{definition}[Histogram notation, multiset version]
\label{defn:histogram}
    Let $h_x: \mathcal{X} \rightarrow \mathbb{N}$ be the histogram of a multiset $x \in \MultiSet(\mathcal{X} )$ for some domain $\mathcal{X}$. That is, $h_x(z)$ denotes the number of occurrences of $z \in \mathcal{X}$ in multiset $x$ (with multiplicities).
\end{definition}

%\todo{Change to vector domain notation. For vector $v$, $h_v(z)$ denotes the number of occurrences of $z$ in $\MultiSet(v)$.} \todo{Specify types: z?}

\begin{definition}[Histogram notation, vector version]
    For any vector $v$ of elements of a domain \texttt{D}, $h_v$ denotes the histogram of $v$. That is, for every element $z$ of type \texttt{T}, $h_v(z)$ denotes the number of occurrences of $z \in \texttt{D}$ in the entries of vector $v$ (with multiplicities).
\end{definition}
\fi

\begin{remark}
    We remark that there is a bijection between multisets and histograms. Therefore, a proof can be carried out with either representation, although usually one will be simpler.
\end{remark}

\begin{definition}[Histogram notation, generic version]
\label{defn:histogram}
    Let $h_x: \mathcal{X} \rightarrow \mathbb{N}$ be the histogram of an (ordered or unordered) list $x$, where every element $\ell\in x$ is drawn from domain $\mathcal{X}$. That is, $h_x(z)$ denotes the number of occurrences of every $z \in \mathcal{X}$ in list $x$ (with multiplicities).
\end{definition}

%Given a vector $v$ in some vector domain, $h_v(z)$ denotes the number of occurrences of $z$ in $\MultiSet(v)$.

\section{Transformations \& Stability relations}
%\silvia{Transformations here? Section for them?}
\begin{definition}[Transformation]
    A transformation $T$ is a \textit{deterministic} mapping from arbitrary data types of derived data values to arbitrary data types of derived data values. In Rust, a transformation is specified by the following attributes: input domain, output domain, function, input metric, output metric, and stability relation. 
\end{definition}

See \href{https://www.overleaf.com/project/60d215bf90b337ac02200a99}{``List of definitions used in the pseudocode"} for further details of the pseudocode specification of a transformation.

%\silvia{Not c in R}
\begin{definition}[Stability parameter]\label{def:c}
    For some value of $c$, we say that a transformation $T$ is $c$\textit{-stable} if for all $x, x'$ in the input domain $\X$, and for input metric $d_{\X}$ and output metric $d_{\Y}$,
    \begin{equation}
        d_{\mathcal{Y}}(T(x), T(x')) \leq c \cdot d_{\mathcal{X}}(x, x').
    \end{equation}
    We say that $c$ is the \textit{stability parameter} of $T$. 
\end{definition}

\connor{Should we include the remark below?}

\begin{remark}[Lipschitz constant]
    Note that the $c$-stable transformation $T$ in \ref{def:c} is also called a Lipschitz function with Lipschitz constant $c$. The stability parameter is analogous to the Lipschitz constant.
\end{remark}

The stability relation is a more robust relationship compared to the Lipschitz function and constant defined in other programming frameworks defined in Fuzzi and PinQ \grace{TODO cite these?}. There are two advantages for the notion of a stability relation over the linear lipschitz function. First, $\Relation$ can capture a non-linear inequality, such as $\Relation(\din, \dout) = (\dout \leq \din^2)$. Second, the $\Relation$ can capture the composition of more numbers for $\din$ or $\dout$ respectively. We can define $\din$ as multiple privacy loss parameters in a composition. \grace{Composition has not been implemented yet in Prog Framework.}
\grace{Even though we said that not all stability relations can be expressed only with linear relationship, we don't have a more general definition that encompasses broader notions.}

\begin{example}
    In the case of scalar clamp, the stability relation is $\dout \geq \min(\din, U - L)$. Hence, the relationship between $\dout$ and $\din$ is not linear.
\end{example}

In the Rust code, the stability parameter $c$ (which in the case of clamping is equal to~1) gets wrapped up inside of the stability relation property, and the end user can test it empirically.

\begin{definition}[Linear stability relation]
    The \textit{linear stability relation}, denoted by the term $\Relation(\din, \dout)$, is a boolean function which takes as input some $\din, \dout$ appropriately quantified and returns \texttt{True} if and only if the relation $\dout \geq c \cdot \din$ for some specified value of $c$. 
\end{definition}

We can also write the stability relation in the following form:

\begin{equation}
    \Relation(\din, \dout) = 
    \begin{cases} 
      \True & \textrm{if } \dout \geq c \cdot \din \\
      \False & \textrm{otherwise},
   \end{cases}
\end{equation}
specifying the concrete metrics $d_{\X}, d_{\Y}$ and types of $\din, \dout$ inside the function.

The associated type of $d_{\X}$ is equal to the type of $\din$, and the associated type of $d_{\Y}$ is equal to the type of $\dout$. Importantly, such relations are sound but \textit{not necessarily complete}. A transformation is considered \textit{valid} if its stability relation is sound.

\subsection{Stability for Row Transforms}
The following lemma can be applied to row by row transformations. This simplification allows us to avoid case by case analysis of the entire vector by focusing on function on the single row itself. 

\begin{definition}[Pure Function]
    A pure function is a function that has the following properties:
    \begin{itemize}
        \item The function return values are identical for identical arguments.
        \item The function application has no side effects.
    \end{itemize}
\end{definition}

Note: This function is taken from the \href{https://en.wikipedia.org/wiki/Pure_function}{``Pure function'' article on Wikipedia}. \grace{The definition will need to be modified according to Marco's comments.} 

\silvia{Also need to add the definition of randomized pure function?}

\begin{definition}[$\Vect$]
    We say that vector $v$ is drawn from domain $\Vect(\mathcal{X})$ if and only if all elements $v_i\in v$ are from domain $\mathcal{X}$.
\end{definition}

\begin{definition}[Row transform]
Let $f: \Vect(\mathcal{X})\rightarrow \Vect(\mathcal{Y})$ be a function on vectors, and let $v = [v_1,\ldots,v_n]$ be a vector of $n$ elements. We say that $f$ is a row transform with respect to a function $g:\mathcal{X}\rightarrow\mathcal{Y}$ if and only if $f(v) = [g(v_1), \ldots, g(v_n)]$.
\end{definition}

\begin{remark}
    Intuitively, a row by row transformation works like \texttt{map} function in CS. It is a higher-order function that applies a given function to each row (or element) of the function. 
\end{remark}

\begin{example}[$isEqual$ is a row by row transform]
Let $bool$ be the domain of boolean values. For every vector $v = [v_1,\ldots,v_n]\in\Vect(\mathcal{X})$, $isEqual(v,val): \Vect(\mathcal{X})\times \mathbb{R} \rightarrow \Vect(bool)$ is defined as $isEqual(v,val) = [v_1 == val, \ldots, v_n == val]$.

Because $isEqual$ applies the function $f(input)  = (input == val)$ to every element of the vector, it is a row-by-row transform.
\end{example}

\begin{lemma}[Symmetric Distance of Row Transform]
Let $f$ be a row transform. For every pair of vectors $v, w\in \Vect(\mathcal{X})$, we have
$$d_{Sym}(f(v), f(w)) \leq d_{Sym}(v, w).$$
\end{lemma}

\begin{proof}
    We use the histogram notation. Recall that $h_{f(v)}(z)$ is the number of occurrences of $z$ in vector $f(v)$. This is equivalent to the sum of the number of occurrences of each $y \in f^{(-1)}(z)$ in vector $v$. Since $h_{\texttt{function}(v)}(z) = \sum_{y \in f^{-1}(z)} h_v(y)$, we have:

\begin{align*}
    \abs{h_{f(v)}(z) - h_{f(w)}(z)} &= \abs{\sum_{y \in f^{-1}(z)} h_v(y) - h_w(y)}\\
    &\leq \sum_{y \in f^{-1}(z)} \abs{h_v(y) - h_w(y)}.
\end{align*}

The last inequality follows by the triangle inequality. To compute the symmetric distance, we have to sum over all possible elements $z$, and then apply the inequality from above:

\begin{align*}
    d_{Sym}(f(v), f(w)) &= 
    \sum_z \abs{h_{f(v)}(z) - h_{f(w)}(z)} \\
    &\leq \sum_z \sum_{y \in f^{-1}(z)} \abs{h_v(y) - h_w(y)}\\
    % &= \sum_y \abs{h_{v}(y) - h_{w}(y)} \\
    % &= d_{Sym}(v, w)
\end{align*}

Note that because the sets $f^{-1}(z)$ form a partition of the domain of $f$, we can simply sum over elements $y$ in the domain of $f$: $$\sum_z \sum_{y \in f^{-1}(z)} \abs{h_v(y) - h_w(y)} = \sum_y \abs{h_{v}(y) - h_{w}(y)} = d_{Sym}(v, w)$$

Therefore we have $$d_{Sym}(f(v), f(w)) \leq d_{Sym}(v, w),$$ as desired.
\end{proof}

\questionc{Add any other metrics?}

\silvia{Proof is in \texttt{make\_row\_by\_row}. But there is a problem with the role of the pure function there.} 

\begin{remark}
    If the \texttt{make\_row\_by\_row} transformation is directly invoked in the code (as opposed to only using the \textit{row transform} as a proof strategy), then we require that $f$ is a pure function. One such example would be the Impute Constant transformation.
\end{remark}

%\begin{definition}[Relation]
%Relation($d_{in}$, $d_{out}$) means that \grace{Silvia can you add what you had for Relation here?}
%\end{definition}

\horizline

\subsection{Notes, todos, questions}

%\todo{Add explanation on forward and backward map?}

%\silvia{TODO: define first the customary definition for sym dist}
%\silvia{TODO: and make it correct for multisets}

\section{Metrics}
Whenever a metric is defined, this document will contain its mathematical definition. In turn, the document \href{https://www.overleaf.com/project/60d215bf90b337ac02200a99}{``List of definitions used in the pseudocode"} will include the list of compatible domains, the list of associated types, and the definition of $d$-close under said metric.

%\silvia{Very important to always specify the types and domains when presenting metrics! (remarks from 24/6 Prof. Vadhan's OH) So:}

%\silvia{With that, changed multisets to vectors, but have we made it clear enough that this is how OpenDP is representing multisets? E.g., $u \Delta v$ is the multiset vs $u \Delta v$ is the vector corresponding to... (see below)}

%\silvia{From meeting on July 13: each metric definition must contain its associated type and the list of possible domains it works with. This is not explicitly specified in Rust (i.e., the metric is only a name), but every time we find a transformation / measurement which uses the metric, include the corresponding domain in the definition if it is not there already. It should also include what it means to be $\din$ close under that metric.}

%\connor{Because the Rust type on which a metric operates is covered in the pseudocode definitions, I think we just need to talk mathematically here, like ``symmetric distance is defined on vectors''.}

%\silvia{On July 15 we settled on only writing the mathematical definition in this document, and then writing the Rust list of domains and associated types in the pseudocode definitions document.}

\begin{definition}[Associated type]
    The associated type of any input metric is the type of the corresponding $\din$. In turn, the associated type of any output metric is the type of the corresponding $\dout$.
\end{definition}

\begin{remark}
    Whenever a metric is defined, there is only an associated type, while there are no input/output type. In the OpenDP programming framework, we do not think of metrics as functions in the usual sense; instead, the associated type of distance is the type of the corresponding $\din$ or $\dout$.
\end{remark}

\subsection{Dataset metrics}
\subsubsection{Symmetric distance}

\questionc{What should we do to eliminate one of the two definitons? Should we remove anything else?}

\begin{definition}[Symmetric difference]
The \textit{symmetric difference} between any two vectors $u, v$, denoted by $\MultiSet(u)\Delta \MultiSet(v)$, corresponds to the multiset representation of elements which are in either $u$ or $v$ but not in their intersection. The multiplicity of each element $x$ in $\MultiSet(u)\Delta \MultiSet(v)$ corresponds to the difference in absolute value of the multiplicities of $x$ in $\MultiSet(u)$ and in $\MultiSet(v)$.

%\connor{I think we should have an explanation of what is means for an element to be in, for example, $u$ but not in $v$. For example, if we have $u = \MultiSet(0,0,1)$ and $v = \MultiSet(0)$, is $0$ in $v$ or not? We want it to be that ``the first $0$ is in $v$, but the second $0$ is not in $v$'', but I don't think the current definition gets that message across.}

%\todo{Improve this definition}
\end{definition}

%\todo{Changed my mind after preconditions: better to say symmetric distance than \texttt{SymmetricDistance} (no longer appears in pseudocode)}

%\todo{Still unclear which information should be part of the metric. Mike said on 7/6 not to include the list of possible domains, but then where do we say $u, v$ belong to? Mike also said that he is not sure whether the \texttt{u32} should be part of the definition.}

\begin{example}
Because a multiset can have repeated elements, for $a = \MultiSet(1,2,1)$ and $b = \MultiSet(1,3)$, we have $a\Delta b = \MultiSet(1,3, 2)$. 
\end{example}

We introduce the notion of symmetric distance, which differs from symmetric difference in that it is the \emph{cardinality} of the multiset instead of the multiset itself.

\begin{remark}
    Symmetric \emph{distance} is the metric which is used in the OpenDP library, while the definition for symmetric \emph{difference} is only included for completeness in this document.
\end{remark}

\begin{remark}[Inspiration for symmetric distance]
    The notion of \emph{symmetric distance} was inspired by the idea of calculating the cardinality of the symmetric difference between multisets. That is, the symmetric distance between any two (ordered or unordered) lists $u, v$, denoted $d_{Sym}(u,v) = |\MultiSet(u) \Delta \MultiSet(v)|$, can be thought of as the cardinality of the symmetric difference between the multiset interpretations of lists $u$ and $v$. 
    
    Because there is a bijection between histograms and multisets, the symmetric distance between vectors $u$ and $v$ can also be thought of as the $L1$ distance between the histograms for $u$ and $v$, denoted $h_u$ and $h_v$, where by abuse of notation, $h_u$ and $h_v$ represent the  (see Definition \ref{defn:histogram}). Then, we equivalently obtain that the symmetric distance between $u$ and $v$ is
    $$d_{\text{Sym}}(u,v) = \lVert h_{u} - h_{v}\rVert_1 = \sum_{z\in \mathcal{X}} |h_u(z) - h_{v}(z)|.$$
    
    The second equality follows from the definition of $L1$ distance (see definition \ref{def:l1-dist}).
    
    We now proceed with the definition of symmetric distance, which takes these inspirations and generalizes them.
\end{remark}

% \begin{definition}[Symmetric distance]
% The \textit{symmetric distance} between any two (ordered or unordered) lists $u, v$, denoted $d_{Sym}(u,v) = |\MultiSet(u) \Delta \MultiSet(v)|$, is equal to the cardinality of the symmetric difference between the multiset interpretations of lists $u$ and $v$.
% \end{definition}

% % still working on this paragraph
% Because there is a bijection between histograms and multisets, we can also define the \emph{symmetric distance} between vectors $u$ and $v$ as the $L1$ distance between the histograms for $u$ and $v$, denoted $h_u$ and $h_v$ (see Definition \ref{defn:histogram}). Then, we equivalently obtain that the symmetric distance between $u$ and $v$ is
% $$d_{\text{Sym}}(u,v) = \lVert h_{u} - h_{v}\rVert_1 = \sum_{z\in \mathcal{X}} |h_u(z) - h_{v}(z)|.$$

% The second equality follows from the definition of $L1$ distance (see definition \ref{def:l1-dist}).

\begin{definition}[Symmetric distance]

The symmetric distance between two (unordered or ordered) lists $u,v$, with every element in these lists drawn from domain $\mathcal{X}$, is

\begin{equation}
    d_{Sym}(u,v)=\sum_{z\in \mathcal{X}} |h_u(z)- h_v(z)|.
\end{equation}
    
\end{definition}

\begin{claim}
Symmetric distance is a metric.
\end{claim}

Note that null data values are still counted in the symmetric distance. Adding or removing null values still influences the count.

\subsubsection{Substitute distance}
\begin{definition}[Substitute distance]
We only define the \emph{substitute distance} $d_{Sym}$ on multisets with the same number of elements. On two multisets $u,v\in \MultiSet(\mathcal{X})$ for some domain $\mathcal{X}$ where $|u| = |v|$, we say that the substitute distance $d_{Subs}(u,v)$ is equal to the cardinality of the relative complement $u \backslash v$, so $d_{Subs}(u,v) = |u\backslash v|$. (This can be thought of as fixing multiset $u$ and finding how many elements in $v$ are not represented in $v$.) \silvia{List of domains? Associated types? It is not in the library, so we cannot answer this -- maybe we should drop it.}

\connor{Substitute distance should have the same domain as symmetric distance since it is defined as half the symmetric distance. Probably the same ``associated type'', too, but I don't think we need to cover that here.}

Alternatively, we can define $d_{Subs}$ as

$$d_{Subs}(u,v) = \frac{1}{2}d_{\text{Sym}}(u,v).$$

Note that this metric is like a generalization of Hamming distance to multisets (recall that Hamming distance is defined on ordered objects). \questionc{Is Hamming distance included in the library again after our discussions on August 12?}
\end{definition}
\silvia{I would not define $d_{Subs}$ in terms of $d_{Sym}$; and instead write original definition and then add the 2-factor equality as a claim.}
\connor{I think they had defined $d_{subs}$ in terms of $d_{sym}$.} \questionc{How is it thought of as in the code? Is it literally ``we calculate the substitute distance, }

\begin{claim}
Substitute distance is a metric.
\end{claim}

%\begin{claim}
%(2 factor of Subs and Sym)
%\end{claim}

\begin{remark}
    As of June 24, symmetric distance is a preferred metric over substitute distance. There is a constructor that converts the metric.
\end{remark}

\subsection{Sensitivity metric}
\subsubsection{Absolute distance}

\begin{definition}[Absolute distance]\label{def:abs}
    Given two numbers $n, m$, the absolute distance between $n$ and $m$, denoted by $d_{Abs}$, is defined as $d_{Abs}(n, m)= |n-m|$, where the horizontal bars represent absolute value.
\end{definition}

The same definition holds for $\dout$.

\begin{claim}
    Absolute distance is a metric.
\end{claim}

\subsubsection{L1 distance}
\begin{definition}
\label{def:l1-dist}
    The \textit{L1 distance} between any two vectors $u, v$, denoted by $d_{L1}(u, v)$, is defined as $d_{L1}(u, v) = \sum_{i=0}^n |u_i - v_i|$.
\end{definition}

\subsubsection{L2 distance}
\begin{definition}
    The \textit{L2 distance} between any two vectors $u, v$, denoted by $d_{L2}(u, v)$, is defined as $d_{L2}(u, v) = \sqrt{\sum_{i=0}^n |u_i - v_i|^2}$.
\end{definition}

More generally, we can define the $L_p$ distance between two vectors.

\subsection{Closeness}
\begin{definition}[$k$-close]
For any metric $d$, we say that two elements $u, v$ are $k$-close under $d$ if $d(u, v) \leq k$.
\end{definition}

For example, in the case of symmetric distance, vectors $u$ and $v$ are $k$-close whenever $d_{Sym}(u, v) = |\textrm{MultiSets}(v) \Delta \textrm{MultiSets}(u)| \leq k$. We remark that the type of $k$ must correspond to the associated type of $d$.

We remark that the notion of $k$-closeness can be defined more generally without relying on metrics and instead only using $\din, \dout$ (e.g., $(\epsilon, \delta)$-DP). If that is the case, it will be defined accordingly.

\horizline

\subsection{Notes, todos, questions}

\todo{Make sure to cite Programming Framework when required.}

\horizline

\subsection{Notes, todos, questions}

\horizline

{\Large\color{red} The definitions in sections 5+ are not used in the most finalized proof documents. Please leave feedback, but note that the correctness of the following sections does not impact the correctness of the proofs that are most finalized.}

\section{Useful lemmas for simplifying proofs}

\subsection{Stability for Randomness}

The following lemma and corollary are used to prove the stability guarantee holds in random transformations like \texttt{make\_impute}. 

\begin{definition}[Coupling]
Let $R$ and $R'$ be two random variables defined over the probability spaces $S$ and $S'$, respectively. A \emph{coupling} of $R$ and $R'$ is a joint variable $(r, r')$ taking values in the product space $S \times S'$ such that $r$ has the same marginal distribution as $R$ and $r'$ has the same marginal distribution as $R'.$
\end{definition}
\grace{Taken from Jayshree's paper on Theil-Sen estimator.}

\begin{lemma}[Stability for Randomness]
We define a randomized function $f: \texttt{DI} \rightsquigarrow \texttt{DO}$. $\Relation(d_{in}, d_{out}) = True$ implies that for all $x, x' \in$ \texttt{DI} that are $d_{in}$-close, there exists a coupling $(R, R')$ of the randomness of $f(x)$ and $f(x')$ such that for all $(r, r') \in Support(R, R'),$ $f_r(x)$ is $d_{out}$-close to $f_{r'}(x')$.
\end{lemma}

\begin{proof}
\grace{Proof outline:}
\begin{itemize}
    \item Jayshree's paper shows how to do it for neighboring inputs $x, x'$, then $T$ is c-Lipschitz randomized transformation. More precisely, she says that if $dist(d_0, d_1) = 1$, then $dist(T(d_0), T(d_1)) \leq c$.
    \item We can generalize for any neighboring inputs: $d_Y(T(d_0), T(d_1)) \leq c \times d_X(d_0, d_1)$. 
    \item Use the fact that $c$-stable implies Relation($\din, \dout$) = $\dout \geq c \times \din$, $d_X(x, x') \leq \din, d_Y(T(x), T(x')) \leq \dout.$
    \item Then apply Lemma 2.0.8 Jayshree's paper. Important things to note are that we find the probability distribution, which then shows that there exists a valid coupling of $s_{d_0}, s_{d_l}$. This allows us to conclude by triangle inequality distance between the coupled transformations dist$(\tilde{s}_{d_0}$, $\tilde{s}_{d_l}) \leq c \times l$. Therefore then $T$ is a c-Lipschitz randomized transformation.
    \item If $T$ is a c-Lipschitz randomized transformation, that means that $f_r(x)$ is $\dout$-close to $f_{r'}(x')$. \grace{Need to flesh out these details.}
\end{itemize}

The following corollary allows us to fix the random seed in random transformations to prove the stability guarantee.
\end{proof}

\begin{corollary}
For randomized function $f: \texttt{DI} \rightsquigarrow \texttt{DO}$, $\Relation(d_{in}, d_{out}) = \texttt{True}$ implies that for all $x, x' \in$ \texttt{DI} that are $d_{in}$-close and for all fixing of the randomness $r$ of $f$ (fixing seed of PRG), we have that $f_r(x)$ and $f_r(x')$ are $d_{out}$-close.
\end{corollary}

\subsection{The path property of symmetric distance on sized domains}

\questionc{Given that we are avoiding texttt notation in this document, should we change \texttt{SizedDomain} for a mathematical definition of sized domain in the three lemmas that follow?}

\begin{definition}[$SizedDomain$]
    $SizedDomain(\mathcal{X},n)$ is the set of all vectors with $n$ (not necessarily unique) elements drawn from domain $\mathcal{X}$.
\end{definition}

\begin{lemma}[Path property of $d_{Sym}$ on \textit{SizedDomain}]\label{lemma:path1}
    For any two vectors $v, w \in$ \textit{SizedDomain(D, n)} for some domain $D$ and integer $n$, $d_{Sym}(v,w)$ is an even integer; i.e., $d_{Sym}(v,w) = 2k$ for some integer $k \geq 0$. Moreover, there exist $k$ vectors $v=u^0, u^1, \ldots, u^k=w$ such that $d_{Sym}(u^i,u^{i+1})=2$ for all $i$.
\end{lemma}

\begin{proof}
    We will first show that $d_{Sym}(v, w) = 2k$ for some integer $k \geq 0$. Since $v, w \in \textit{SizedDomain(D, n)}$, we know that both $v$ and $w$ have length $n$. Let $v[i], w[i]$ denote the $i$-th element of $v$ and $w$, respectively, for $i \geq 0$. We construct $v', w'$ with the following iterative algorithm \texttt{Alg1}: 
    \begin{enumerate}
        \item Set $v' \leftarrow v$ and $w' \leftarrow w$.
        \item For each index $i \in [0, n-1]$, do:
        \begin{enumerate}
            \item (Deleting step.) Delete elements $v'[i]$ and $w'[j]$ if and only if there exists a $j \in [0, n-1]$ such that $v'[i] = w'[j]$.
        \end{enumerate}
        \item Return $v'$ and $w'$.
    \end{enumerate}
    When \texttt{Alg1} terminates, it must be that $\MultiSet(v') \cap \MultiSet(w') = \emptyset$; otherwise, there would exist an index $i$ such that $v'[i] = w'[j]$ for some $j$, which contradicts step 2 (a) in \texttt{Alg1}. Moreover, at each deleting step of the iterative algorithm, we delete exactly one element of $v'$ and one element of $w'$. Let $k'$ denote the total number of deletion steps. Since by assumption $|\MultiSet(v)| = |\MultiSet(w)| = n$, it follows that
    \[
        |\MultiSet(v')| = |\MultiSet(w')| = n-k'.
    \]
    Hence, $d_{Sym}(v, w) = 2k$, where $k = n-k'$.
    
    Next we prove the second part of the lemma by using the same iterative algorithm. We show by construction how to find the $k$ vectors $v = u^0, u^1, \ldots, u^k = w$ such that $d_{Sym}(u^i, u^{i+1}) = 2 \forall \, i$. Let $D$ denote \silvia{Letter already taken} the vector of indices $j$ (in increasing order) such that $v[j]$ has not been deleted at any stage during the iterative algorithm described above As shown above, we know that $|D| = k$. We now construct the $k$ vectors $u^i$ with the following \texttt{Alg2}:
    \begin{enumerate}
        \item Set $u^0 \leftarrow v$.
        \item For each $i \in [1, k]$, do:
        \begin{enumerate}
            \item Set $u^i \leftarrow u^{i-1}$.
            \item Set $u^i[D[i]] \leftarrow w[D[i]]$.
        \end{enumerate}
    \end{enumerate}
\end{proof}
We need to prove two claims: that at the end of \texttt{Alg2}, $u^k = w$, and that $d_{Sym}(u^i, u^{i+1}) = 2$ for all $i$. \silvia{Problem of ordering. Needs finishing.}

\silvia{Notation is bad. Maybe change to $v[i]$ to denote $i$-th element of $v$.}

\begin{lemma}\label{lemma:path2}
    For any two vectors $v, w \in$ \texttt{SizedDomain(D, n)}, $d_{Sym}(v, w) = 2$ if and only if we can change one element of $v$ to obtain $v'$ such that $\MultiSet(v') = \MultiSet(w)$.
\end{lemma}

\begin{proof}
    This lemma follows directly form the definition of symmetric distance. We need to prove the two directions of the implication:
    \begin{enumerate}
        \item If we can change one element of $v$ to obtain $v'$ such that $\MultiSet(v') = \MultiSet(w)$, then $d_{Sym}(v, w) = 2$: This direction is already shown in the proof of Lemma 4.1.
        \item If $d_{Sym}(v, w) = 2$, then we can change one element of $v$ to obtain $v'$ such that $\MultiSet(v') = \MultiSet(w)$: By the histogram notation...
    \end{enumerate}
\end{proof}

The above lemma follows directly from the definition of symmetric distance. 

\begin{lemma}\label{lemma:path3}
Given a function $f: \mathcal{X} \rightarrow \mathcal{Y}$ on input domain \texttt{SizedDomain(D, n)}, if $d_{\mathcal{Y}}(f(v), f(w)) \leq c$ for all vectors $v, w \in \texttt{SizedDomain(D, n)}$ such that $d_{Sym}(v, w) = 2$, then $f$ is $c/2$-stable.
\end{lemma}

\begin{proof}
    Triangle inequality.
\end{proof}

By the definition of $c$-stable (Definition~\ref{def:c}), 
that $f$ is $c/2$-stable is equivalent to stating that for all pairs $v', w'$ in the input domain, the following holds:
\[
    d_{\Y}(f(v'), f(w')) \leq c/2 \cdot d_{Sym}(v', w').
\]
Also equivalently, this means that the stability relation of $f$ is
\[
    \dout \geq c/2 \cdot \din.
\]

\subsection{The path property: a generalization}
Shortest path metric on a weighted graph with positive (non-zero) edge weights. 

\begin{lemma}[Generalized path property]
    \silvia{Generalization of Lemma \ref{lemma:path1}}
    Given an arbitrary domain-metric pair $(D, d_M)$, we say that $(D, d_M)$ has the path property if there exists a mapping $f: D \rightarrow V$, where $G(V, E)$ denotes a graph with  positive non-zero edge weights and associated shortest path metric $d_G$, such that $\forall v, w \in D$, $d_M(v, w) = d_G(f(v), f(w))$.
\end{lemma}

\silvia{We can either say ``$(D, M)$ has the path property" or ``a metric on a domain has the path property".}

\silvia{For lemma below, define the equivalent of $d_{Sym}=2$ as the least possible value of $d_M$ or as the distance between two adjacent vertices in the associated graph? Because $\MultiSet$ is specific to $d_{Sym}$. Or change to $d_G(f(v), f(w)) = d_{M_{\min}}$?}

\questionc{Next lemma should work without the path property but I would need a general notion of adjacency; e.g., ``if and only if $v, w$ are adjacent elements". Redundant with $d_{M_{\min}}$? How to generalize the definitions correctly?}

\begin{lemma}
    \silvia{Generalization of Lemma \ref{lemma:path2}}
    For any domain-metric pair $(D, d_M)$ with the path property and hence with an associated graph $G(V, E)$, let $d_{M_{\min}}$ be the minimum possible value of $d_M(v, w)$ over of all possible input pairs $v, w \in D$ for $v \neq w$. Then, $\forall v, w \in D$, $d_M(v, w) = d_{M_{\min}}$ if and only if $f(v), f(w)$ are adjacent vertices in $G$.
    %$v, w$ are adjacent elements in $D$ under metric $d_M$.
\end{lemma}

We say that two elements $v, w \in D$ are \textit{adjacent} if and only if $d_M(v, w) = d_{M_{\min}}$.

No need to require the path property in Lemma 4.5 above (and hence have a graph) if $v, w$ adjacent $\rightarrow$ $f(v), f(w)$ adjacent.

\silvia{It is important to specify that $d_{M_{\min}}$ is minimal \textit{over $D$.} E.g., adjacent datasets are at $d_{Sym} = 1$ for unbounded domain, but at $d_{Sym} = 2$ for a bounded domain.}

%\questionc{Version 1:}
%\begin{lemma}
    %\silvia{Generalization of Lemma 4.3, Version 1.}
    For any domain-metric pair $(D, d_M)$ with the path property and hence with an associated graph $G(V, E)$, and for any function $f: D \rightarrow \mathcal{Y}$, where domain $\mathcal{Y}$ has associated metric $d_{\mathcal{Y}}$, if $d_{\mathcal{Y}}(f(v), f(w)) \leq c$ for all $v, w \in D$ such that $f(v), f(w)$ are adjacent vertices in $G$, then $f$ is $c/d_{M_{\min}}$-stable.
%\end{lemma}

%\questionc{Version 2: (the two versions should be equivalent precisely by Lemma 4.5, but I like Version 2 better because it does not need Lemma 4.5}
\begin{lemma}
    \silvia{Generalization of Lemma \ref{lemma:path3} %, Version 2.
    }
    For any domain-metric pair $(D, d_M)$ with the path property and hence with an associated graph $G(V, E)$, and for any function $f: D \rightarrow \mathcal{Y}$, where domain $\mathcal{Y}$ has associated metric $d_{\mathcal{Y}}$, if $d_{\mathcal{Y}}(f(v), f(w)) \leq c$ for all $v, w \in D$ such that $d_M(u, v) = d_{M_{\min}}$, then $f$ is $c/d_{M_{\min}}$-stable.
\end{lemma}

How to generalize $d_{Sym} = 2$? There's 3 possibilities ($d_{Sym} \rightarrow d_M$):
\begin{itemize}
    \item $d_M(u, v) = d_{M_{\min}}$
    \item $u,v$ are adjacent elements under metric $d_M$ (does this make any sense?)
    \item $f(u), f(v)$ are adjacent vertices in $G$ (depends on the specific construction?)
\end{itemize}

\todo{Here add how the symmetric difference case follows from section 4.1.3 by constructing the graph explicitly.}


\section{Measures}
\subsection{Max divergence}
\begin{definition}[Max divergence \cite{dr14}]
    The max divergence between two random variables $Y$ and $Z$ taking values from the same domain is defined to be:
    \[
        D_{\infty}(Y||Z) = \max_{S \subseteq \textrm{Supp}(Y)} \Big[\ln \dfrac{\Pr[Y \in S]}{\Pr[Z \in S]} \Big].
    \]
\end{definition}

\begin{lemma}[\cite{dr14}]
    A mechanism $\M$ is $\epsilon$-differentially private if and only if on every two neighboring databases $x$ and $y$, $D_{\infty}(\M(x)||\M(y)) \leq \epsilon$ and $D_{\infty}(\M(y)||\M(x)) \leq \epsilon$.
\end{lemma}

\subsection{Smoothed max divergence}
\begin{definition}[Smoothed max divergence \cite{dr14}]
    The smoothed max divergence between $Y$ and $Z$ is defined to be:
    \[
        D^{\delta}_{\infty}(Y||Z) = \max_{S \subseteq \textrm{Supp}(Y): \Pr[Y \in S] \geq \delta} \Big[\ln \dfrac{\Pr[Y \in S]-\delta}{\Pr[Z \in S]} \Big].
    \]
\end{definition}

\begin{lemma}[\cite{dr14}]
    A mechanism $\M$ is $(\epsilon, \delta)$-differentially private if and only if on every two neighboring databases $x$ and $y$, $D_{\infty}^{\delta}(\M(x)||\M(y)) \leq \epsilon$ and $D_{\infty}^{\delta}(\M(y)||\M(x)) \leq \epsilon$.
\end{lemma}


\section{Probability distributions}
\subsection{Laplace distribution}
\begin{definition}[Laplace distribution \cite{dr14}]
    The Laplace distribution (centered at 0) with scale $b$ is the distribution with pribability density function
    \[
        \Lap(x|b) = \dfrac{1}{2b} \exp \Big(\dfrac{-|x|}{b}\Big).
    \]
    The variance of this distribution is $\sigma^2 = 2b^2$. \silvia{Add cumulative def.? Probably not necessary. Discuss anything else we would like to add.}
\end{definition}
We write $\Lap(b)$ to denote the Laplace distribution with scale $b$.

\silvia{Connor, Grace: can you add Geometric and Gaussian?}

\horizline

\subsection{Notes, todos, questions}

\section{Floating point approach}

\begin{thebibliography}{99}

\bibitem{dr14}
Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. \textit{Found. Trends Theor. Comput. Sci.} 9.3--4 (2014): 211--407.


\end{thebibliography}


\end{document}
